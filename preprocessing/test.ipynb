{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Introduction\n",
    "This notebook implements the complete pipeline for:\n",
    "1. Loading and preprocessing Empatica E4 wearable data\n",
    "2. Feature engineering for sleep stage classification\n",
    "3. Quality control and data cleaning\n",
    "4. Preparing data for models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Setup and Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import os\n",
    "import json\n",
    "from tqdm import tqdm\n",
    "import neurokit2 as nk\n",
    "from scipy import signal\n",
    "from scipy.interpolate import interp1d\n",
    "from scipy.stats import skew\n",
    "from collections import Counter\n",
    "import warnings\n",
    "from multiprocessing import Pool, cpu_count\n",
    "\n",
    "# Configuration\n",
    "DATA_FOLDER = \"data/datasets\"\n",
    "FEATURES_DIR = \"data/features_df\"\n",
    "INFO_FILE = \"data/participant_info.csv\"\n",
    "QUALITY_FILE = \"data/quality_scores_per_subject.csv\"\n",
    "THRESHOLD = 0.2  # Quality threshold\n",
    "\n",
    "# Set display options\n",
    "pd.set_option(\"display.max_rows\", 500)\n",
    "pd.set_option(\"display.max_columns\", 20)\n",
    "pd.set_option(\"display.width\", 1000)\n",
    "\n",
    "# Ignore warnings\n",
    "warnings.filterwarnings(\"ignore\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Data Loading and Preprocessing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Core Preprocessing Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preprocess_BVP(bvp):\n",
    "    \"\"\"Preprocess BVP signal with Chebyshev filter\"\"\"\n",
    "    sos = signal.cheby2(N=10, rs=40, Wn=[0.5, 15], btype=\"bandpass\", fs=64, output=\"sos\")\n",
    "    return signal.sosfilt(sos, bvp)\n",
    "\n",
    "def preprocess_ACC(acc, fs=32):\n",
    "    \"\"\"Bandpass filter accelerometer data\"\"\"\n",
    "    sos = signal.butter(N=3, Wn=[3, 10], btype=\"bp\", fs=fs, output=\"sos\")\n",
    "    return signal.sosfilt(sos, acc)\n",
    "\n",
    "def preprocess_EDA(eda, fs=4):\n",
    "    \"\"\"Detrend and filter EDA signal\"\"\"\n",
    "    # Detrend in 5-second segments\n",
    "    detrended = []\n",
    "    for i in range(len(eda)//20):\n",
    "        segment = eda[i*20:(i+1)*20]\n",
    "        m, b = np.polyfit(np.arange(20), segment, 1)\n",
    "        detrended.append(segment - (m*np.arange(20) + b)\n",
    "    \n",
    "    # Filter\n",
    "    sos = signal.butter(N=3, Wn=0.7, fs=fs, output=\"sos\")\n",
    "    return signal.sosfilt(sos, np.concatenate(detrended))\n",
    "\n",
    "def preprocess_ALL_SIGNALS(df):\n",
    "    \"\"\"Preprocess all signals in a dataframe\"\"\"\n",
    "    processed_df = df.copy()\n",
    "    \n",
    "    # Process each signal\n",
    "    processed_df[\"BVP\"] = preprocess_BVP(df[\"BVP\"])\n",
    "    processed_df[\"ACC_X\"] = preprocess_ACC(df[\"ACC_X\"])\n",
    "    processed_df[\"ACC_Y\"] = preprocess_ACC(df[\"ACC_Y\"])\n",
    "    processed_df[\"ACC_Z\"] = preprocess_ACC(df[\"ACC_Z\"])\n",
    "    processed_df[\"EDA\"] = preprocess_EDA(df[\"EDA\"])\n",
    "    \n",
    "    # Process timestamps\n",
    "    ts = df['TIMESTAMP'].to_numpy()\n",
    "    ts = ts - ts[0]\n",
    "    processed_df[\"TIMESTAMP_COSINE\"] = circadian_cosine(ts)\n",
    "    processed_df[\"TIMESTAMP_DECAY\"] = circadian_decay(ts)\n",
    "    processed_df[\"TIMESTAMP_LINEAR\"] = circadian_linear(ts)\n",
    "    \n",
    "    return processed_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 Circadian Rhythm Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def circadian_cosine(ts, samp_freq=64):\n",
    "    \"\"\"Calculate circadian cosine feature\"\"\"\n",
    "    len_vec = (ts[-1] - ts[0]) * samp_freq\n",
    "    return np.sin((2 * np.pi / (len_vec * 2)) * np.arange(len(ts)))\n",
    "\n",
    "def circadian_decay(ts, samp_freq=64):\n",
    "    \"\"\"Calculate circadian decay feature\"\"\"\n",
    "    len_vec = (ts[-1] - ts[0]) * samp_freq\n",
    "    k = np.log(0.01) / len_vec\n",
    "    return np.exp(k * np.arange(len(ts)))\n",
    "\n",
    "def circadian_linear(ts, samp_freq=64):\n",
    "    \"\"\"Calculate circadian linear feature\"\"\"\n",
    "    len_vec = (ts[-1] - ts[0]) * samp_freq\n",
    "    return np.arange(len(ts)) / len_vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Feature Extraction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Signal Quality Assessment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def exclude_signal(segment_df):\n",
    "    \"\"\"Identify if a segment should be labeled as artifact\"\"\"\n",
    "    bvp = segment_df.BVP.to_numpy()\n",
    "    bvp = bvp - np.mean(bvp)\n",
    "    \n",
    "    # Filter and calculate SNR\n",
    "    b, a = signal.butter(2, [0.5/32, 15/32], btype=\"band\")\n",
    "    filtered = signal.filtfilt(b, a, bvp)\n",
    "    snr_db = 10 * np.log10(np.mean(filtered**2) / np.mean((bvp-filtered)**2))\n",
    "    \n",
    "    # Calculate activity index\n",
    "    acc = np.sqrt(segment_df[['ACC_X','ACC_Y','ACC_Z']].mean(axis=1))\n",
    "    acc_std = np.std(acc)\n",
    "    \n",
    "    # Rule-based artifact detection\n",
    "    if (acc_std >= 0.4125/2) or (snr_db < 10) or \\\n",
    "       (np.max(bvp) > 500) or (np.min(bvp) < -500):\n",
    "        return 1\n",
    "    return 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 HRV Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "HRV_FEATURES = [\n",
    "    \"HRV_MeanNN\", \"HRV_SDNN\", \"HRV_RMSSD\", \"HRV_pNN50\",\n",
    "    \"HRV_LF\", \"HRV_HF\", \"HRV_LFHF\", \"HRV_SD1\", \"HRV_SD2\"\n",
    "]\n",
    "\n",
    "def extract_hrv_features(segment_df):\n",
    "    \"\"\"Extract HRV features using NeuroKit2\"\"\"\n",
    "    try:\n",
    "        signals, info = nk.ppg_process(segment_df.BVP, sampling_rate=64)\n",
    "        results = nk.ppg_analyze(signals, sampling_rate=64)\n",
    "        return {f: results[f].iloc[0] for f in HRV_FEATURES}, 0\n",
    "    except:\n",
    "        return {f: np.nan for f in HRV_FEATURES}, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.3 Accelerometer Features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_acc_features(segment_df):\n",
    "    \"\"\"Extract accelerometer features\"\"\"\n",
    "    acc_x = segment_df.ACC_X.to_numpy()\n",
    "    acc_y = segment_df.ACC_Y.to_numpy()\n",
    "    acc_z = segment_df.ACC_Z.to_numpy()\n",
    "    acc_mag = np.sqrt(acc_x**2 + acc_y**2 + acc_z**2)\n",
    "    \n",
    "    # Trimmed statistics (ignore top/bottom 10%)\n",
    "    acc_trimmed = acc_mag[(acc_mag > np.quantile(acc_mag, 0.1)) & \n",
    "                         (acc_mag < np.quantile(acc_mag, 0.9))]\n",
    "    \n",
    "    return {\n",
    "        \"ACC_mean\": np.mean(acc_trimmed),\n",
    "        \"ACC_max\": np.max(acc_trimmed),\n",
    "        \"ACC_iqr\": np.percentile(acc_trimmed, 75) - np.percentile(acc_trimmed, 25),\n",
    "        \"ACC_std\": np.std(acc_mag)\n",
    "    }"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.4 Complete Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_domain_features(sid, data_folder, save_folder_dir, segment_seconds=30):\n",
    "    \"\"\"Extract all features for one subject\"\"\"\n",
    "    # Load and preprocess data\n",
    "    df = pd.read_csv(f\"{data_folder}/{sid}_whole_df.csv\")\n",
    "    df = preprocess_ALL_SIGNALS(df)\n",
    "    \n",
    "    # Initialize feature storage\n",
    "    epoch_length = segment_seconds * 64\n",
    "    num_segments = len(df) // epoch_length\n",
    "    features = []\n",
    "    \n",
    "    # Process each epoch\n",
    "    for i in tqdm(range(num_segments), desc=f\"Processing {sid}\"):\n",
    "        segment = df.iloc[i*epoch_length:(i+1)*epoch_length]\n",
    "        \n",
    "        # Extract features\n",
    "        hrv_feats, _ = extract_hrv_features(segment)\n",
    "        acc_feats = extract_acc_features(segment)\n",
    "        \n",
    "        # Combine all features\n",
    "        epoch_feats = {**hrv_feats, **acc_feats}\n",
    "        epoch_feats['artifact'] = exclude_signal(segment)\n",
    "        epoch_feats['sid'] = sid\n",
    "        features.append(epoch_feats)\n",
    "    \n",
    "    # Save features\n",
    "    features_df = pd.DataFrame(features)\n",
    "    features_df.to_csv(f\"{save_folder_dir}/{sid}_domain_features_df.csv\", index=False)\n",
    "    return features_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Quality Control and Data Preparation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.1 Calculate Quality Scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_quality_scores(feature_path, output_path):\n",
    "    \"\"\"Calculate quality scores for all subjects\"\"\"\n",
    "    files = [f for f in os.listdir(feature_path) if f.endswith('_domain_features_df.csv')]\n",
    "    results = []\n",
    "    \n",
    "    for file in tqdm(files, desc=\"Calculating quality scores\"):\n",
    "        sid = file.split('_')[0]\n",
    "        df = pd.read_csv(f\"{feature_path}/{file}\")\n",
    "        perc_excluded = df['artifact'].mean()\n",
    "        results.append({\n",
    "            'sid': sid,\n",
    "            'total_segments': len(df),\n",
    "            'num_excludes': df['artifact'].sum(),\n",
    "            'percentage_excludes': perc_excluded\n",
    "        })\n",
    "    \n",
    "    quality_df = pd.DataFrame(results)\n",
    "    quality_df.to_csv(output_path, index=False)\n",
    "    return quality_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5.2 Prepare Modeling Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def prepare_modeling_data(threshold=0.2):\n",
    "    \"\"\"Prepare final dataset for modeling\"\"\"\n",
    "    # Load quality scores and filter subjects\n",
    "    quality_df = pd.read_csv(QUALITY_FILE)\n",
    "    good_sids = quality_df[quality_df.percentage_excludes < threshold].sid.tolist()\n",
    "    \n",
    "    # Load and concatenate all feature files\n",
    "    dfs = []\n",
    "    for sid in tqdm(good_sids, desc=\"Loading feature files\"):\n",
    "        df = pd.read_csv(f\"{FEATURES_DIR}/{sid}_domain_features_df.csv\")\n",
    "        dfs.append(df)\n",
    "    \n",
    "    full_df = pd.concat(dfs)\n",
    "    \n",
    "    # Clean data\n",
    "    full_df = full_df.replace([np.inf, -np.inf], np.nan)\n",
    "    full_df = full_df.dropna(axis=1, thresh=0.8*len(full_df))  # Drop columns with >20% NA\n",
    "    full_df = full_df.dropna()  # Drop remaining rows with NA\n",
    "    \n",
    "    # Save cleaned data\n",
    "    os.makedirs(\"data/processed\", exist_ok=True)\n",
    "    full_df.to_csv(\"data/processed/clean_df.csv\", index=False)\n",
    "    \n",
    "    # Get final feature list\n",
    "    features = [col for col in full_df.columns if col not in \n",
    "               ['sid', 'Sleep_Stage', 'artifact']]\n",
    "    \n",
    "    return full_df, features, good_sids"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Running the Complete Pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6.1 Process All Subjects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_all_subjects():\n",
    "    \"\"\"Run complete processing pipeline\"\"\"\n",
    "    # Create directories\n",
    "    os.makedirs(FEATURES_DIR, exist_ok=True)\n",
    "    \n",
    "    # Get list of subjects to process\n",
    "    sid_list = [f.split('_')[0] for f in os.listdir(DATA_FOLDER) \n",
    "               if f.endswith('_whole_df.csv')]\n",
    "    \n",
    "    # Process each subject\n",
    "    for sid in tqdm(sid_list, desc=\"Processing subjects\"):\n",
    "        if not os.path.exists(f\"{FEATURES_DIR}/{sid}_domain_features_df.csv\"):\n",
    "            try:\n",
    "                extract_domain_features(\n",
    "                    sid=sid,\n",
    "                    data_folder=DATA_FOLDER,\n",
    "                    save_folder_dir=FEATURES_DIR,\n",
    "                    segment_seconds=30\n",
    "                )\n",
    "            except Exception as e:\n",
    "                print(f\"Error processing {sid}: {str(e)}\")\n",
    "    \n",
    "    # Calculate quality scores\n",
    "    calculate_quality_scores(FEATURES_DIR, QUALITY_FILE)\n",
    "    \n",
    "    # Prepare final modeling dataset\n",
    "    clean_df, features, good_sids = prepare_modeling_data(threshold=THRESHOLD)\n",
    "    \n",
    "    print(\"\\nPipeline completed successfully!\")\n",
    "    print(f\"Final dataset contains {len(clean_df)} epochs from {len(good_sids)} subjects\")\n",
    "    print(f\"Number of features: {len(features)}\")\n",
    "    \n",
    "    return clean_df, features, good_sids\n",
    "\n",
    "# Uncomment to run the complete pipeline\n",
    "# clean_df, features, good_sids = process_all_subjects()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
